import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import warnings
import os
import json
import time
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import Dataset, DataLoader

# --- ê¸°ë³¸ ì„¤ì • ---
# GPU/CUDA ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
warnings.filterwarnings("ignore")
model_dir = "./saved_models"

# ==========================================
# 1. Dataset í´ë˜ìŠ¤ (ëˆ„ë½ë˜ì—ˆë˜ ë¶€ë¶„)
# ==========================================
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ==========================================
# 2. AI ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (LSTM, GRU)
# ==========================================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output

class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out[:, -1, :])
        output = self.fc(gru_out)
        return output

# ==========================================
# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ëª¨ë¸ ë¡œë“œ, ì „ì´í•™ìŠµ)
# ==========================================
def load_jeju_pretrained_models(model_dir='./saved_models', timestamp=None):
    """ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°"""
    if timestamp is None:
        latest_path = os.path.join(model_dir, 'latest_models.json')
        if not os.path.exists(latest_path):
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜ˆì™¸ ì²˜ë¦¬ ëŒ€ì‹  None ë°˜í™˜í•˜ê±°ë‚˜ ë¹ˆ ê»ë°ê¸° ìƒì„± ë¡œì§ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ì—ëŸ¬ë¥¼ ë„ì›ë‹ˆë‹¤.
            raise FileNotFoundError(f"ìµœì‹  ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_path}")
        with open(latest_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        timestamp = model_info['timestamp']
    
    metadata_path = os.path.join(model_dir, f'metadata_{timestamp}.json')
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    lstm_path = os.path.join(model_dir, f'lstm_model_{timestamp}.pth')
    lstm_checkpoint = torch.load(lstm_path, map_location=device, weights_only=False)
    lstm_model = LSTMModel(**lstm_checkpoint['model_config']).to(device)
    lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
    
    gru_path = os.path.join(model_dir, f'gru_model_{timestamp}.pth')
    gru_checkpoint = torch.load(gru_path, map_location=device, weights_only=False)
    gru_model = GRUModel(**gru_checkpoint['model_config']).to(device)
    gru_model.load_state_dict(gru_checkpoint['model_state_dict'])
    
    return lstm_model, gru_model, metadata

def transfer_learning(model, train_loader, val_loader, criterion, 
                     num_epochs=10, patience=3, learning_rate=0.0001, 
                     freeze_layers=False, device='cpu', model_name='Model'):
    """ì „ì´í•™ìŠµ (Fine-tuning) ìˆ˜í–‰"""
    print(f"   >> {model_name} í•™ìŠµ ì‹œì‘ (LR: {learning_rate})...")
    
    if freeze_layers:
        for param in model.parameters():
            param.requires_grad = False
        for param in model.fc.parameters():
            param.requires_grad = True
    else:
        for param in model.parameters():
            param.requires_grad = True
    
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)
    
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = model.state_dict().copy()
    
    for epoch in range(num_epochs):
        model.train()
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                break
    
    model.load_state_dict(best_model_state)
    return model, [], []

def predict_future(model, scaler_X, scaler_y, last_sequence, target_datetime, solar_capacity, device='cpu'):
    """ë‹¨ì¼ ì‹œì  ë¯¸ë˜ ì˜ˆì¸¡"""
    model.eval()
    
    # UTC -> KST ë³€í™˜í•˜ì—¬ ì‹œê°„ ê³„ì‚°
    from datetime import timezone, timedelta
    kst_tz = timezone(timedelta(hours=9))
    
    # target_datetimeì´ UTCë¼ë©´ KSTë¡œ ë³€í™˜
    if target_datetime.tzinfo is None:
        target_kst = target_datetime  # timezone ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    else:
        target_kst = target_datetime.astimezone(kst_tz)
    
    month = target_kst.month
    hour = target_kst.hour  # â† KST ê¸°ì¤€ ì‹œê°„!
    
    # ê³„ì ˆë³„/ì‹œê°„ëŒ€ë³„ ê¸°ìƒ íŒ¨í„´ ìƒì„± (ê°„ëµí™”)
    if month in [11, 12, 1, 2]: base_temp, base_humid, base_cloud = 5, 60, 5
    elif month in [3, 4, 5]: base_temp, base_humid, base_cloud = 15, 55, 4
    elif month in [6, 7, 8]: base_temp, base_humid, base_cloud = 25, 70, 6
    else: base_temp, base_humid, base_cloud = 15, 65, 5
    
    if 6 <= hour <= 12: temperature = base_temp + (hour - 6) * 1.5
    elif 12 < hour <= 18: temperature = base_temp + 9 - (hour - 12) * 1.0
    else: temperature = base_temp - 3
    
    # KST ê¸°ì¤€ìœ¼ë¡œ ë‚®/ë°¤ íŒë‹¨
    if 6 <= hour <= 18:
        sunshine_duration = 0.8 if 9 <= hour <= 15 else 0.3
        solar_radiation = 600 if 9 <= hour <= 15 else 200
    else:
        sunshine_duration = 0
        solar_radiation = 0
    
    # ëª¨ë¸ í•™ìŠµ ë•Œ ì‚¬ìš©í•œ íŠ¹ì„± ìˆœì„œ ì¤€ìˆ˜
    new_features = np.array([[
        temperature, 0, base_humid, base_cloud,
        sunshine_duration, solar_radiation, solar_capacity, hour
    ]])
    
    new_features_scaled = scaler_X.transform(new_features)
    new_sequence = np.vstack([last_sequence[1:], new_features_scaled])
    new_sequence_tensor = torch.FloatTensor(new_sequence).unsqueeze(0).to(device)
    
    with torch.no_grad():
        prediction_scaled = model(new_sequence_tensor).cpu().numpy()
        prediction = scaler_y.inverse_transform(prediction_scaled)[0, 0]
    
    return max(0, prediction), new_sequence

# ==========================================
# 4. ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (Inferenceìš© vs Trainingìš©)
# ==========================================
def load_data_from_db(df, sequence_length=24):
    """[ì¶”ë¡ ìš©] DB ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
    df_renamed = df.copy()
    if 'datetime' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['datetime'])
    elif 'ë°œì „ì¼ì' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['ë°œì „ì¼ì'])
    
    cols_to_fill_zero = ['precipitation', 'snow', 'sunshine_duration', 'solar_radiation']
    for col in cols_to_fill_zero:
        if col in df_renamed.columns:
            df_renamed[col] = df_renamed[col].fillna(0)
            
    df_renamed['hour'] = df_renamed['datetime'].dt.hour
    
    feature_cols = [
        'temperature', 'precipitation', 'humidity', 'cloud_cover',
        'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour'
    ]
    
    target_col = 'solar_generation'
    # ì¶”ë¡  ì‹œì—ëŠ” íƒ€ê²Ÿê°’ì´ ì—†ì–´ë„ ë˜ì§€ë§Œ, ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•´ dropnaë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•¨
    # ì—¬ê¸°ì„œëŠ” generation ë°ì´í„°ê°€ ìˆëŠ” êµ¬ê°„ë§Œ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •
    df_valid = df_renamed.dropna(subset=[target_col]).copy()
    
    X = df_valid[feature_cols].values
    y = df_valid[target_col].values.reshape(-1, 1)
    
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    X_seq = []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
    
    X_seq = np.array(X_seq)
    
    # 5ê°œ ê°’ ë°˜í™˜ (ì¶”ë¡ ìš©)
    return X_seq, scaler_X, scaler_y, feature_cols, df_valid

def load_train_data_from_db(df, sequence_length=24):
    """[ì¬í•™ìŠµìš©] ë°ì´í„° ì „ì²˜ë¦¬ ë° Train/Val/Test ë¶„í•  í•¨ìˆ˜"""
    df_renamed = df.copy()
    if 'datetime' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['datetime'])
    
    cols_to_fill_zero = ['precipitation', 'snow', 'sunshine_duration', 'solar_radiation']
    for col in cols_to_fill_zero:
        if col in df_renamed.columns:
            df_renamed[col] = df_renamed[col].fillna(0)

    df_renamed['hour'] = df_renamed['datetime'].dt.hour
    
    feature_cols = [
        'temperature', 'precipitation', 'humidity', 'cloud_cover',
        'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour'
    ]
    
    target_col = 'solar_generation'
    df_valid = df_renamed.dropna(subset=[target_col]).copy()
    
    X = df_valid[feature_cols].values
    y = df_valid[target_col].values.reshape(-1, 1)
    dates = df_valid['datetime'].values
    
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    X_seq, y_seq, date_seq = [], [], []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
        y_seq.append(y_scaled[i+sequence_length])
        date_seq.append(dates[i+sequence_length])
    
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    if len(X_seq) < 10:
        return [], [], [], [], [], [], scaler_X, scaler_y, feature_cols, df_valid, []

    X_temp, X_test, y_temp, y_test, date_temp, date_test = train_test_split(
        X_seq, y_seq, date_seq, test_size=0.1, random_state=42
    )
    X_train, X_val, y_train, y_val, date_train, date_val = train_test_split(
        X_temp, y_temp, date_temp, test_size=0.111, random_state=42
    )
    
    # 11ê°œ ê°’ ë°˜í™˜ (í•™ìŠµìš©)
    return (X_train, X_val, X_test, y_train, y_val, y_test,
            scaler_X, scaler_y, feature_cols, df_valid, date_test)

# ==========================================
# 5. Main Entry Points (Celery Taskì—ì„œ í˜¸ì¶œ)
# ==========================================
def run_prediction(df_input, loaded_models=None):
    """
    [1ì‹œê°„ ì£¼ê¸°] ì˜ˆì¸¡ ì‹¤í–‰ í•¨ìˆ˜
    """
    try:
        # 1. ëª¨ë¸ ë¡œë“œ
        if loaded_models:
            lstm_model, gru_model, _ = loaded_models
        else:
            lstm_model, gru_model, _ = load_jeju_pretrained_models(model_dir="./saved_models")
        
        SEQUENCE_LENGTH = 24
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬ (ì¶”ë¡ ìš© í•¨ìˆ˜ ì‚¬ìš©)
        if df_input.empty:
            return []
            
        X_seq, scaler_X, scaler_y, _, df_valid = load_data_from_db(df_input, SEQUENCE_LENGTH)
        
        if len(X_seq) == 0:
            print("ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return []

        # 3. ë¯¸ë˜ ì˜ˆì¸¡ (ê°€ì¥ ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€)
        current_time = df_valid['datetime'].iloc[-1]
        solar_capacity = df_valid['solar_capacity'].iloc[0]
        last_sequence = X_seq[-1]
        
        all_predictions = []
        temp_sequence = last_sequence.copy()
        
        # 72ì‹œê°„ ì˜ˆì¸¡ ìˆ˜í–‰
        for h in range(1, 73):
            target_time = current_time + timedelta(hours=h)
            
            lstm_pred, temp_sequence = predict_future(
                lstm_model, scaler_X, scaler_y, temp_sequence,
                target_time, solar_capacity, device
            )
            gru_pred, _ = predict_future(
                gru_model, scaler_X, scaler_y, temp_sequence,
                target_time, solar_capacity, device
            )
            
            ensemble_pred = (lstm_pred + gru_pred) / 2
            
            all_predictions.append({
                'ì˜ˆì¸¡ì¼ì‹œ': target_time,
                'ì•™ìƒë¸”_ë°œì „ëŸ‰(MWh)': max(0, ensemble_pred)
            })

        return all_predictions

    except Exception as e:
        print(f"ì˜ˆì¸¡ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return []

def retrain_model(df_train):
    """
    [í•˜ë£¨ 1ë²ˆ] ì¬í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
    """
    try:
        print("\nğŸš€ [Model Retraining] ì‹œì‘...")
        
        # 1. ëª¨ë¸ ë¡œë“œ
        lstm_model, gru_model, metadata = load_jeju_pretrained_models(model_dir=model_dir)
        
        SEQUENCE_LENGTH = 24
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        (X_train, X_val, X_test, y_train, y_val, y_test,
         scaler_X, scaler_y, feature_cols, df_valid, date_test) = load_train_data_from_db(df_train, SEQUENCE_LENGTH)
        
        if len(X_train) == 0:
            print("âš ï¸ í•™ìŠµí•  ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
            return False

        # 3. DataLoader ìƒì„±
        BATCH_SIZE = 32
        train_dataset = TimeSeriesDataset(X_train, y_train)
        val_dataset = TimeSeriesDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
        
        # 4. ì „ì´í•™ìŠµ ìˆ˜í–‰
        criterion = nn.MSELoss()
        
        lstm_model, _, _ = transfer_learning(
            lstm_model, train_loader, val_loader, criterion,
            num_epochs=10, patience=3, learning_rate=0.0001,
            freeze_layers=False, device=device, model_name='LSTM'
        )
        
        gru_model, _, _ = transfer_learning(
            gru_model, train_loader, val_loader, criterion,
            num_epochs=10, patience=3, learning_rate=0.0001,
            freeze_layers=False, device=device, model_name='GRU'
        )
        
        # ==========================================
        # 5. ëª¨ë¸ ì €ì¥ (ì—ëŸ¬ ìˆ˜ì • ë¶€ë¶„)
        # ==========================================
        new_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì— configê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš© (ì•ˆì „ì¥ì¹˜)
        default_config = {
            'input_size': 8,     # íŠ¹ì„± 8ê°œ
            'hidden_size': 128,  # ëª¨ë¸ ê¸°ë³¸ê°’
            'num_layers': 2,
            'dropout': 0.2
        }
        
        # .get()ì„ ì¨ì„œ í‚¤ê°€ ì—†ìœ¼ë©´ default_configë¥¼ ê°€ì ¸ì˜¤ê²Œ í•¨
        lstm_config = metadata.get('lstm_config', default_config)
        gru_config = metadata.get('gru_config', default_config)
        
        # LSTM ì €ì¥
        torch.save({
            'model_state_dict': lstm_model.state_dict(),
            'model_config': lstm_config
        }, os.path.join(model_dir, f'lstm_model_{new_timestamp}.pth'))
        
        # GRU ì €ì¥
        torch.save({
            'model_state_dict': gru_model.state_dict(),
            'model_config': gru_config
        }, os.path.join(model_dir, f'gru_model_{new_timestamp}.pth'))
        
        # ë©”íƒ€ë°ì´í„° ê°±ì‹  (ë‹¤ìŒë²ˆì—” ì—ëŸ¬ ì•ˆ ë‚˜ë„ë¡ configë„ ê°™ì´ ì €ì¥)
        metadata['timestamp'] = new_timestamp
        metadata['lstm_config'] = lstm_config
        metadata['gru_config'] = gru_config
        
        with open(os.path.join(model_dir, f'metadata_{new_timestamp}.json'), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=4)
            
        with open(os.path.join(model_dir, 'latest_models.json'), 'w', encoding='utf-8') as f:
            json.dump({'timestamp': new_timestamp}, f, indent=4)
            
        print(f"âœ… ì¬í•™ìŠµ ì™„ë£Œ! ìƒˆë¡œìš´ ëª¨ë¸ ë²„ì „: {new_timestamp}")
        return True

    except Exception as e:
        print(f"âŒ ì¬í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False